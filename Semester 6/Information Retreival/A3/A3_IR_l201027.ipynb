{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Read Graph Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_graph_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    vertex_map = {}\n",
    "    for line in lines:\n",
    "        for vertex in line.split():\n",
    "            if vertex not in vertex_map:\n",
    "                vertex_map[vertex] = len(vertex_map)\n",
    "\n",
    "    num_vertices = len(vertex_map)\n",
    "    adj_matrix = np.zeros((num_vertices, num_vertices))\n",
    "    for line in lines:\n",
    "        vertices = line.split()\n",
    "        from_vertex_idx = vertex_map[vertices[0]]\n",
    "        to_vertex_indices = [vertex_map[v] for v in vertices[1:]]\n",
    "        for to_vertex_idx in to_vertex_indices:\n",
    "            adj_matrix[from_vertex_idx, to_vertex_idx] = 1\n",
    "\n",
    "    return adj_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Page Rank and Markov Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.33333333 0.33333333 0.33333333 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.5        0.5       ]\n",
      " [0.         0.25       0.         0.25       0.25       0.25      ]\n",
      " [0.33333333 0.33333333 0.         0.         0.33333333 0.        ]\n",
      " [0.5        0.         0.         0.5        0.         0.        ]\n",
      " [0.33333333 0.33333333 0.         0.         0.33333333 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# PART (A)\n",
    "M = read_graph_file('graph.txt')\n",
    "\n",
    "row_sums = M.sum(axis=1)\n",
    "\n",
    "P = M / row_sums[:, np.newaxis]\n",
    "\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2115869  0.19143577 0.07052897 0.19647355 0.21662469 0.11335013]\n"
     ]
    }
   ],
   "source": [
    "# PART (B)\n",
    "A = np.vstack((P.T - np.eye(6), np.ones(6)))\n",
    "\n",
    "b = np.zeros(7)\n",
    "b[-1] = 1\n",
    "\n",
    "pi = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20559467 0.1905434  0.08412452 0.19183791 0.20877479 0.11912471]\n",
      "Number of iterations: 3\n"
     ]
    }
   ],
   "source": [
    "# PART (C)\n",
    "d = 0.85\n",
    "\n",
    "N = len(P)\n",
    "P_prime = (1-d)/N * np.ones((N, N)) + d * P\n",
    "\n",
    "pi_0 = np.array([1/6, 1/6, 1/6, 1/6, 1/6, 1/6])\n",
    "\n",
    "max_abs_diff = np.inf\n",
    "n = 0\n",
    "while max_abs_diff > 0.01:\n",
    "    n += 1\n",
    "    pi_n = pi_0.dot(P_prime)\n",
    "    max_abs_diff = np.max(np.abs(pi_n - pi_0))\n",
    "    pi_0 = pi_n\n",
    "\n",
    "print(pi_0)\n",
    "print(\"Number of iterations:\", n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Iterative Page Rank Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(M, num_iterations=100, d=0.85):\n",
    "    N = M.shape[1]\n",
    "    v = np.random.rand(N, 1)\n",
    "    v = v / np.linalg.norm(v, 1)\n",
    "    M_hat = (d * M + (1 - d) / N)\n",
    "    for i in range(num_iterations):\n",
    "        v = M_hat @ v\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.13079785e+37]\n",
      " [1.33049328e+37]\n",
      " [2.46980370e+37]\n",
      " [1.88762061e+37]\n",
      " [1.53974427e+37]\n",
      " [1.88762061e+37]]\n"
     ]
    }
   ],
   "source": [
    "M = read_graph_file('graph.txt')\n",
    "\n",
    "scores = pagerank(M)\n",
    "print(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Running the algo on given file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = read_graph_file('wt2g_inlinks.txt')\n",
    "\n",
    "scores = pagerank(M)\n",
    "print(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Web Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidSchema",
     "evalue": "No connection adapters were found for 'mailto:abbya@mit.edu'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-fdb5b850ca05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# crawl the URL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m     \u001b[0mcrawl_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;31m# wait for the specified delay before making the next HTTP request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-fdb5b850ca05>\u001b[0m in \u001b[0;36mcrawl_url\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;31m# make an HTTP request for the URL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'User-Agent'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUSER_AGENT\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;31m# check if the response is an HTML document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nadeem\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \"\"\"\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"get\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nadeem\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nadeem\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    585\u001b[0m         }\n\u001b[0;32m    586\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nadeem\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m         \u001b[1;31m# Get the appropriate adapter to use\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 695\u001b[1;33m         \u001b[0madapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[1;31m# Start time (approximately) of the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nadeem\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mget_adapter\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[1;31m# Nothing matches :-/\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mInvalidSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"No connection adapters were found for {url!r}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidSchema\u001b[0m: No connection adapters were found for 'mailto:abbya@mit.edu'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "MAX_PAGES = 100\n",
    "\n",
    "DOMAIN = 'mit.edu'\n",
    "\n",
    "USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
    "\n",
    "DELAY = 5\n",
    "\n",
    "OUTPUT_FILE = 'output.txt'\n",
    "\n",
    "ROBOTS_TXT_URL = 'https://www.mit.edu/robots.txt'\n",
    "\n",
    "visited_urls = set()\n",
    "\n",
    "urls_to_visit = ['https://www.mit.edu']\n",
    "\n",
    "def is_valid_domain(url):\n",
    "    return DOMAIN in url\n",
    "\n",
    "def canonicalize_url(url):\n",
    "    url = url.split('#')[0]\n",
    "    url = url.split('?')[0]\n",
    "    url = url.rstrip('/')\n",
    "    if '.' not in url.split('/')[-1]:\n",
    "        url += '/'\n",
    "    return url\n",
    "\n",
    "def is_allowed_by_robots(url):\n",
    "    response = requests.get(ROBOTS_TXT_URL, headers={'User-Agent': USER_AGENT})\n",
    "    robots_txt = response.text\n",
    "    parsed_robots_txt = RobotFileParser()\n",
    "    parsed_robots_txt.parse(robots_txt.splitlines())\n",
    "    return parsed_robots_txt.can_fetch(USER_AGENT, url)\n",
    "    \n",
    "def crawl_url(url):\n",
    "    global visited_urls\n",
    "    global urls_to_visit\n",
    "    \n",
    "    if url in visited_urls:\n",
    "        return\n",
    "    if not is_valid_domain(url):\n",
    "        return\n",
    "    if not is_allowed_by_robots(url):\n",
    "        return\n",
    "\n",
    "    visited_urls.add(url)\n",
    "\n",
    "    response = requests.get(url, headers={'User-Agent': USER_AGENT})\n",
    "\n",
    "    if 'text/html' not in response.headers['Content-Type']:\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    canonical_url = canonicalize_url(response.url)\n",
    "\n",
    "    outgoing_links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href')\n",
    "        if href is not None and is_valid_domain(href):\n",
    "            outgoing_links.append(canonicalize_url(href))\n",
    "\n",
    "    with open(OUTPUT_FILE, 'a') as f:\n",
    "        f.write(canonical_url + ' ' + ' '.join(outgoing_links) + '\\n')\n",
    "\n",
    "    for link in outgoing_links:\n",
    "        if link not in visited_urls:\n",
    "            urls_to_visit.append(link)\n",
    "\n",
    "while len(visited_urls) < MAX_PAGES and len(urls_to_visit) > 0:\n",
    "    url = urls_to_visit.pop(0)\n",
    "\n",
    "    crawl_url(url)\n",
    "\n",
    "    time.sleep(DELAY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
