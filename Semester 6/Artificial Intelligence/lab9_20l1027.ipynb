{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r'C:\\Users\\Nadeem\\Desktop\\BSDS\\Semester 6\\Artificial Intelligence\\flowers'\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "image_size = (64, 64)\n",
    "\n",
    "for folder_name in os.listdir(folder_path):\n",
    "    subfolder_path = os.path.join(folder_path, folder_name)\n",
    "    for file_name in os.listdir(subfolder_path):\n",
    "        file_path = os.path.join(subfolder_path, file_name)\n",
    "        img = Image.open(file_path)\n",
    "        img = img.resize(image_size)\n",
    "        img_array = np.array(img)\n",
    "        data.append(img_array)\n",
    "        labels.append(folder_name)\n",
    "\n",
    "X = np.array(data)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[141 141 139]\n",
      "   [149 148 149]\n",
      "   [152 152 154]\n",
      "   ...\n",
      "   [163 163 167]\n",
      "   [155 155 154]\n",
      "   [152 152 151]]\n",
      "\n",
      "  [[136 136 133]\n",
      "   [146 145 145]\n",
      "   [163 162 167]\n",
      "   ...\n",
      "   [159 158 162]\n",
      "   [156 156 154]\n",
      "   [149 148 148]]\n",
      "\n",
      "  [[129 128 122]\n",
      "   [139 140 136]\n",
      "   [154 154 155]\n",
      "   ...\n",
      "   [157 157 157]\n",
      "   [155 155 154]\n",
      "   [141 140 139]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 40  43  21]\n",
      "   [ 41  44  22]\n",
      "   [ 55  57  42]\n",
      "   ...\n",
      "   [128 124 122]\n",
      "   [126 122 119]\n",
      "   [126 123 118]]\n",
      "\n",
      "  [[ 43  46  24]\n",
      "   [ 44  47  25]\n",
      "   [ 52  54  36]\n",
      "   ...\n",
      "   [133 129 128]\n",
      "   [129 125 124]\n",
      "   [128 125 123]]\n",
      "\n",
      "  [[ 44  47  26]\n",
      "   [ 46  48  27]\n",
      "   [ 52  54  35]\n",
      "   ...\n",
      "   [137 133 132]\n",
      "   [133 129 128]\n",
      "   [130 126 125]]]\n",
      "\n",
      "\n",
      " [[[216 218 226]\n",
      "   [216 219 226]\n",
      "   [219 221 228]\n",
      "   ...\n",
      "   [ 18  24  22]\n",
      "   [ 14  16  17]\n",
      "   [  9   9  14]]\n",
      "\n",
      "  [[229 231 239]\n",
      "   [224 227 233]\n",
      "   [220 222 229]\n",
      "   ...\n",
      "   [ 24  29  28]\n",
      "   [ 15  17  18]\n",
      "   [  9   8  13]]\n",
      "\n",
      "  [[229 233 239]\n",
      "   [232 236 243]\n",
      "   [228 233 240]\n",
      "   ...\n",
      "   [ 32  36  34]\n",
      "   [ 15  17  19]\n",
      "   [  9   8  13]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[183 183 189]\n",
      "   [182 182 188]\n",
      "   [182 182 188]\n",
      "   ...\n",
      "   [146 139 129]\n",
      "   [140 135 124]\n",
      "   [171 167 159]]\n",
      "\n",
      "  [[175 176 181]\n",
      "   [187 187 192]\n",
      "   [203 204 209]\n",
      "   ...\n",
      "   [145 138 127]\n",
      "   [142 136 121]\n",
      "   [175 168 159]]\n",
      "\n",
      "  [[214 218 223]\n",
      "   [225 229 235]\n",
      "   [229 233 239]\n",
      "   ...\n",
      "   [145 137 122]\n",
      "   [144 137 119]\n",
      "   [177 167 156]]]\n",
      "\n",
      "\n",
      " [[[117 116 107]\n",
      "   [ 95  87  88]\n",
      "   [105  95  98]\n",
      "   ...\n",
      "   [127 141 122]\n",
      "   [145 154 135]\n",
      "   [145 151 130]]\n",
      "\n",
      "  [[ 92  86  93]\n",
      "   [ 72  55  74]\n",
      "   [ 85  72  86]\n",
      "   ...\n",
      "   [121 135 120]\n",
      "   [116 125 111]\n",
      "   [116 125 108]]\n",
      "\n",
      "  [[ 93  80  97]\n",
      "   [ 81  65  85]\n",
      "   [ 79  71  84]\n",
      "   ...\n",
      "   [105 109 101]\n",
      "   [107 115 101]\n",
      "   [109 117 103]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 23   8  38]\n",
      "   [ 27  10  42]\n",
      "   [ 39  15  45]\n",
      "   ...\n",
      "   [ 39  17  71]\n",
      "   [ 44  20  79]\n",
      "   [ 48  21  82]]\n",
      "\n",
      "  [[ 26  11  40]\n",
      "   [ 26   9  38]\n",
      "   [ 32  12  41]\n",
      "   ...\n",
      "   [ 41  18  74]\n",
      "   [ 45  21  79]\n",
      "   [ 48  21  82]]\n",
      "\n",
      "  [[ 26  10  39]\n",
      "   [ 25  10  38]\n",
      "   [ 24   7  38]\n",
      "   ...\n",
      "   [ 47  19  77]\n",
      "   [ 50  22  81]\n",
      "   [ 52  24  85]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 87  87  63]\n",
      "   [ 71  74  64]\n",
      "   [ 87  85  58]\n",
      "   ...\n",
      "   [107 134 128]\n",
      "   [111 147 167]\n",
      "   [106 146 175]]\n",
      "\n",
      "  [[ 78  79  63]\n",
      "   [ 86  85  65]\n",
      "   [ 85  82  55]\n",
      "   ...\n",
      "   [105 125  94]\n",
      "   [114 141 135]\n",
      "   [108 138 148]]\n",
      "\n",
      "  [[ 67  71  65]\n",
      "   [ 87  86  66]\n",
      "   [ 76  76  61]\n",
      "   ...\n",
      "   [ 89 110  88]\n",
      "   [112 140 143]\n",
      "   [104 134 135]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[129 120  86]\n",
      "   [130 121  86]\n",
      "   [122 115  79]\n",
      "   ...\n",
      "   [ 69  91  45]\n",
      "   [ 65  89  45]\n",
      "   [ 60  81  41]]\n",
      "\n",
      "  [[130 121  89]\n",
      "   [126 116  82]\n",
      "   [126 118  82]\n",
      "   ...\n",
      "   [ 69  89  43]\n",
      "   [ 65  88  45]\n",
      "   [ 71  93  51]]\n",
      "\n",
      "  [[127 118  82]\n",
      "   [134 123  88]\n",
      "   [114 112  76]\n",
      "   ...\n",
      "   [ 77  98  50]\n",
      "   [ 67  88  44]\n",
      "   [ 69  88  47]]]\n",
      "\n",
      "\n",
      " [[[180 171 166]\n",
      "   [205 204 195]\n",
      "   [173 168 157]\n",
      "   ...\n",
      "   [127 137 141]\n",
      "   [165 164 144]\n",
      "   [199 204 183]]\n",
      "\n",
      "  [[197 191 170]\n",
      "   [220 223 207]\n",
      "   [148 153 160]\n",
      "   ...\n",
      "   [152 151 143]\n",
      "   [146 147 118]\n",
      "   [155 163 136]]\n",
      "\n",
      "  [[170 177 193]\n",
      "   [206 212 209]\n",
      "   [105 127 155]\n",
      "   ...\n",
      "   [179 190 157]\n",
      "   [151 180 132]\n",
      "   [189 208 180]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 23  27  12]\n",
      "   [ 44  56  25]\n",
      "   [ 58  54  10]\n",
      "   ...\n",
      "   [ 28  32  18]\n",
      "   [ 33  41  41]\n",
      "   [ 19  31  17]]\n",
      "\n",
      "  [[ 18  32  28]\n",
      "   [ 58  66  35]\n",
      "   [ 38  31   7]\n",
      "   ...\n",
      "   [ 15  23  11]\n",
      "   [ 15  28  13]\n",
      "   [ 19  28  15]]\n",
      "\n",
      "  [[ 26  44  40]\n",
      "   [ 20  30  24]\n",
      "   [  6   8   7]\n",
      "   ...\n",
      "   [ 13  21  12]\n",
      "   [ 16  27  13]\n",
      "   [ 49  47  10]]]\n",
      "\n",
      "\n",
      " [[[ 49  53  47]\n",
      "   [ 79  80  81]\n",
      "   [ 97 105 112]\n",
      "   ...\n",
      "   [116 127 129]\n",
      "   [141 153 165]\n",
      "   [107 117 127]]\n",
      "\n",
      "  [[ 48  49  45]\n",
      "   [ 86  89  95]\n",
      "   [ 94 105 110]\n",
      "   ...\n",
      "   [157 170 181]\n",
      "   [134 149 160]\n",
      "   [125 136 147]]\n",
      "\n",
      "  [[ 75  77  86]\n",
      "   [103 110 119]\n",
      "   [ 91  98 103]\n",
      "   ...\n",
      "   [133 145 155]\n",
      "   [ 97 109 112]\n",
      "   [120 133 145]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[165  77   2]\n",
      "   [ 76  64   6]\n",
      "   [ 28  48   5]\n",
      "   ...\n",
      "   [ 75  83  38]\n",
      "   [ 81  84  23]\n",
      "   [ 62  70  17]]\n",
      "\n",
      "  [[129  64   3]\n",
      "   [ 60  52   9]\n",
      "   [ 51  82  17]\n",
      "   ...\n",
      "   [ 51  59  19]\n",
      "   [ 62  74  15]\n",
      "   [ 54  67  14]]\n",
      "\n",
      "  [[ 68  45   3]\n",
      "   [ 47  57  14]\n",
      "   [ 65  90  19]\n",
      "   ...\n",
      "   [ 38  46   6]\n",
      "   [ 54  66  12]\n",
      "   [ 50  62   6]]]]\n",
      "['daisy' 'daisy' 'daisy' ... 'tulip' 'tulip' 'tulip']\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros(hidden_size)\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros(output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.A1 = self.sigmoid(self.Z1)\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = np.exp(self.Z2) / np.sum(np.exp(self.Z2), axis=1, keepdims=True)\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        m = len(y)\n",
    "\n",
    "        dZ2 = self.A2\n",
    "        dZ2[np.arange(m), y] -= 1\n",
    "        dW2 = np.dot(self.A1.T, dZ2)\n",
    "        db2 = np.sum(dZ2, axis=0)\n",
    "        dZ1 = np.dot(dZ2, self.W2.T) * self.sigmoid_derivative(self.Z1)\n",
    "        dW1 = np.dot(X.T, dZ1)\n",
    "        db1 = np.sum(dZ1, axis=0)\n",
    "\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "\n",
    "    def train(self, X, y, learning_rate=0.1, epochs=1000, verbose=True):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "        y = y.astype(int)\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.forward(X)\n",
    "            loss = self.loss(y)\n",
    "\n",
    "            if verbose and epoch % 100 == 0:\n",
    "                print('Epoch:', epoch, 'Loss:', loss)\n",
    "\n",
    "            self.backward(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.forward(X)\n",
    "        return np.argmax(self.A2, axis=1)\n",
    "\n",
    "    def loss(self, y):\n",
    "        y = y.astype(int)\n",
    "        return -np.sum(np.log(self.A2[np.arange(len(y)), y])) / len(y)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-f9350de033dd>:58: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "<ipython-input-15-f9350de033dd>:16: RuntimeWarning: overflow encountered in exp\n",
      "  self.A2 = np.exp(self.Z2) / np.sum(np.exp(self.Z2), axis=1, keepdims=True)\n",
      "<ipython-input-15-f9350de033dd>:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.A2 = np.exp(self.Z2) / np.sum(np.exp(self.Z2), axis=1, keepdims=True)\n",
      "<ipython-input-15-f9350de033dd>:55: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.sum(np.log(self.A2[np.arange(len(y)), y])) / len(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 Loss: nan\n",
      "Accuracy: 0.1875\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "X_train_norm = X_train_flat / 255\n",
    "X_test_norm = X_test_flat / 255\n",
    "\n",
    "input_size = X_train_norm.shape[1]\n",
    "hidden_size = 100\n",
    "output_size = len(np.unique(y_train))\n",
    "learning_rate = 0.1\n",
    "epochs = 10\n",
    "\n",
    "mlp = MLP(input_size, hidden_size, output_size)\n",
    "\n",
    "mlp.train(X_train_norm, y_train, learning_rate=learning_rate, epochs=epochs)\n",
    "\n",
    "y_pred = mlp.predict(X_test_norm)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
